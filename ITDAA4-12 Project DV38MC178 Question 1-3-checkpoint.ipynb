{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60f064-87a6-44b6-b1f8-9de5547db024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a directory for data\n",
    "db_directory = \"heart_data\"\n",
    "\n",
    "# Check we have a directory called \"heart_data\" (if not create one)\n",
    "os.makedirs(db_directory, exist_ok=True)\n",
    "\n",
    "# Path to SQLite DB file\n",
    "db_path = os.path.join(db_directory, 'heart_disease.db') \n",
    "\n",
    "# Read CSV file, data will not be read correctly without \";\" delimiter\n",
    "#csv_file_path = r'C:\\Users\\Phillip\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv' (laptop location)\n",
    "csv_file_path = r'C:\\Users\\User\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')  \n",
    "\n",
    "# Show the first few rows to verify a correct reading\n",
    "print(\"DataFrame structure after reading CSV:\")\n",
    "print(df.head())\n",
    "print(\"DataFrame columns and data types after reading CSV:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Create and connect to the DB\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Write the correctly read data frame to the SQLite DB\n",
    "df.to_sql('heart_disease', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Check data in the database is formatted properly  \n",
    "df_sql = pd.read_sql('SELECT * FROM heart_disease', conn)\n",
    "print(\"Data read from the database:\")\n",
    "print(df_sql.head())\n",
    "\n",
    "# Close the connection to the DB\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be9cb7-2685-42be-b98c-1dd3b47d5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('heart_data/heart_disease.db')\n",
    "\n",
    "# Read the data from the database into a DataFrame\n",
    "df = pd.read_sql('SELECT * FROM heart_disease', conn)\n",
    "\n",
    "# Show original values to compare\n",
    "print(\"Original values:\")\n",
    "print(df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']].head())\n",
    "\n",
    "# Find potential missing values in each column\n",
    "print(\"Checking for missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with mean of each column (if there are missing values)\n",
    "print(\"Filling missing values with the mean (if any).\")\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "print(\"Applying one-hot encoding to categorical variables.\")\n",
    "print(df[['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']].head())\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_vars)\n",
    "\n",
    "# Normalize numeric variables in range of 0 to 1\n",
    "print(\"Normalizing numeric variables:\")\n",
    "scaler = MinMaxScaler()\n",
    "numeric_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "\n",
    "# Show the first few rows of the cleaned and preprocessed data frame\n",
    "print(\"Displaying the first few rows of the preprocessed data frame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57327aff-2fb9-40c4-85f8-b8247efc545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Connect to the DB\n",
    "conn = sqlite3.connect(\"heart_data/heart_disease.db\")\n",
    "\n",
    "# Read the data into the data frame \n",
    "df = pd.read_sql('SELECT * FROM heart_disease', conn)\n",
    "\n",
    "# List of the categorical variables  \n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'] \n",
    "\n",
    "# Create bar chart for individual categories by looping through the vars (categories) \n",
    "for var in categorical_vars:\n",
    "    counts = df[var].value_counts()\n",
    "    plt.figure(figsize=(10,5)) \n",
    "    plt.bar(counts.index, counts.values, color ='green') \n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title(f'Frequency of {var}' ) \n",
    "    plt.show() \n",
    "\n",
    "# Close the db connnection \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413870fb-8229-42e3-b6a7-63bd961288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('heart_data/heart_disease.db')\n",
    "\n",
    "# Read the data into the DataFrame\n",
    "df = pd.read_sql('SELECT * FROM heart_disease', conn)\n",
    "\n",
    "# List of categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "#Check exang is just 1 or 0 \n",
    "#print(\"Unique values in 'exang' : \", df['exang'].unique())\n",
    "\n",
    "# Plot the distribution of classes for each categorical variable based on the target variable\n",
    "for var in categorical_vars:\n",
    "    counts = df.groupby(var)['target'].value_counts().unstack()\n",
    "    counts.plot(kind='bar', stacked=True, figsize=(10, 5), color=['red', 'green'])\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Number of people in category')\n",
    "    plt.title(f'Distribution of {var} based on target variable')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='target', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5677a-918b-483b-a421-de4bd73995d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('heart_data/heart_disease.db')\n",
    "\n",
    "# Reads the data into the DataFrame\n",
    "df = pd.read_sql('SELECT * FROM heart_disease', conn)\n",
    "\n",
    "# Creates a fixed array of numeric variables \n",
    "numeric_vars = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Used for looping through the numerical variables for the graph  \n",
    "for var in numeric_vars:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot histogram for target = 0 (no heart disease)\n",
    "    #alpha represents the level of transparency \n",
    "    # bin represents the number of intervals (level of detail the histogram will show) \n",
    "    plt.hist(df[df['target'] == 0][var], bins=15, alpha=0.5, label='No Heart Disease', color='red')\n",
    "    \n",
    "    # Plot histogram for target = 1 (heart disease)\n",
    "    plt.hist(df[df['target'] == 1][var], bins=15, alpha=0.5, label='Heart Disease', color='green')\n",
    "    \n",
    "    plt.xlabel(var)  # Names the x-axis with the related variable name \n",
    "    plt.ylabel('Frequency')  # Names the y-axis 'frequency' for how often it occurs\n",
    "    plt.title(f'Distribution of {var} based on target variable')  # Names the title using related variable being graphed\n",
    "    plt.legend()  # creates a key or legend to increase ease of legibility \n",
    "    plt.show()  # outputs the graph \n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e11138-40c3-44af-ad5e-4c0e61c95182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36162318-507a-40a4-b728-e52a2cf66052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates function to find outliers using interquartile range (IQR) method\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Finds outliers in numerical features\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for feature in numerical_features:\n",
    "    outliers = identify_outliers(df, feature)\n",
    "    print(f\"Outliers in {feature}:\")\n",
    "    print(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832e52-14b3-437e-b03b-dd1e48088cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Creates function to cap outliers using IQT method \n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "\n",
    "#apply caping method to required columbs  \n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for feature in numerical_features:\n",
    "    cap_outliers(df, feature)\n",
    "\n",
    "# Standardizing numerical features after capping outliers\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "# Check data has been processed correctly \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1944de-046e-42d7-8567-969c7dbf3ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file and load it into a DataFrame\n",
    "db_directory = \"heart_data\"\n",
    "os.makedirs(db_directory, exist_ok=True)\n",
    "db_path = os.path.join(db_directory, 'heart_disease.db')\n",
    "#csv_file_path = r'C:\\Users\\Phillip\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv' for laptop \n",
    "csv_file_path = r'C:\\Users\\User\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "print(\"DataFrame structure after reading CSV:\")\n",
    "print(df.head())\n",
    "print(\"DataFrame columns and data types after reading CSV:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values and handle them if needed\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Checking for missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Find and cap outliers\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for feature in numerical_features:\n",
    "    df = cap_outliers(df, feature)\n",
    "\n",
    "# Standardize  numerical features\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "print(\"Check the data has been processed correctly\")\n",
    "print(df.head())\n",
    "\n",
    "#Split  data into training and testing sets\n",
    "# 'X' contains all features,  'y' is the target variable\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "\n",
    "# The test size splits the data into 20% test data and 80% training data\n",
    "#Random state ensures that same results can be replicated\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "# Initialize the Logistic Regression model\n",
    "# Max_iter 35 to ensure that there will be convergence without additional computational strain\n",
    "model = LogisticRegression(max_iter=75)  \n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test model\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Save the model to disk\n",
    "# Set the location that the model can  be saved \n",
    "model_path = os.path.join(db_directory, 'heart_disease_model.pkl')\n",
    "\n",
    "# Save model jus trained\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Check that the model has been saved \n",
    "print(f'Model saved to {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6225d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# Directory setup for data\n",
    "db_directory = \"heart_data\"\n",
    "os.makedirs(db_directory, exist_ok=True)\n",
    "\n",
    "# Path to CSV file \n",
    "csv_file_path = r'C:\\Users\\User\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"DataFrame structure after reading CSV:\")\n",
    "print(df.head())\n",
    "print(\"DataFrame columns and data types after reading CSV:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Checking for missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Find outliers in numerical features\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for feature in numerical_features:\n",
    "    outliers = identify_outliers(df, feature)\n",
    "    print(f\"Outliers in {feature}:\")\n",
    "    print(outliers)\n",
    "\n",
    "# Cap outliers using IQR method\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "# Apply capping method to required columns\n",
    "for feature in numerical_features:\n",
    "    df = cap_outliers(df, feature)\n",
    "\n",
    "# Standardize numerical features after capping outliers\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "# Check the processed data\n",
    "print(\"Check the data has been processed correctly:\")\n",
    "print(df.head())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train decision tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "y_pred = dt_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Save decision model to disk\n",
    "model_path = os.path.join(db_directory, 'heart_disease_dt_model.pkl')\n",
    "joblib.dump(dt_model, model_path)\n",
    "print(f'Model saved to {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc4c1dad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame structure after reading CSV:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n",
      "DataFrame columns and data types after reading CSV:\n",
      "age           int64\n",
      "sex           int64\n",
      "cp            int64\n",
      "trestbps      int64\n",
      "chol          int64\n",
      "fbs           int64\n",
      "restecg       int64\n",
      "thalach       int64\n",
      "exang         int64\n",
      "oldpeak     float64\n",
      "slope         int64\n",
      "ca            int64\n",
      "thal          int64\n",
      "target        int64\n",
      "dtype: object\n",
      "Checking for missing values in each column:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "Outliers in age:\n",
      "Empty DataFrame\n",
      "Columns: [age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, target]\n",
      "Index: []\n",
      "Outliers in trestbps:\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "8     52    1   2       172   199    1        1      162      0      0.5   \n",
      "101   59    1   3       178   270    0        0      145      0      4.2   \n",
      "110   64    0   0       180   325    0        1      154      1      0.0   \n",
      "203   68    1   2       180   274    1        0      150      1      1.6   \n",
      "223   56    0   0       200   288    1        0      133      1      4.0   \n",
      "241   59    0   0       174   249    0        1      143      1      0.0   \n",
      "248   54    1   1       192   283    0        0      195      0      0.0   \n",
      "260   66    0   0       178   228    1        1      165      1      1.0   \n",
      "266   55    0   0       180   327    0        2      117      1      3.4   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "8        2   0     3       1  \n",
      "101      0   0     3       1  \n",
      "110      2   0     2       1  \n",
      "203      1   0     3       0  \n",
      "223      0   2     3       0  \n",
      "241      1   0     2       0  \n",
      "248      2   1     3       0  \n",
      "260      1   2     3       0  \n",
      "266      1   0     2       0  \n",
      "Outliers in chol:\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "28    65    0   2       140   417    1        0      157      0      0.8   \n",
      "85    67    0   2       115   564    0        0      160      0      1.6   \n",
      "96    62    0   0       140   394    0        0      157      0      1.2   \n",
      "220   63    0   0       150   407    0        0      154      0      4.0   \n",
      "246   56    0   0       134   409    0        0      150      1      1.9   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "28       2   1     2       1  \n",
      "85       1   0     3       1  \n",
      "96       1   0     2       1  \n",
      "220      1   3     3       0  \n",
      "246      1   2     3       0  \n",
      "Outliers in thalach:\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "272   67    1   0       120   237    0        1       71      0      1.0   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "272      1   0     2       0  \n",
      "Outliers in oldpeak:\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "101   59    1   3       178   270    0        0      145      0      4.2   \n",
      "204   62    0   0       160   164    0        0      145      0      6.2   \n",
      "221   55    1   0       140   217    0        1      111      1      5.6   \n",
      "250   51    1   0       140   298    0        1      122      1      4.2   \n",
      "291   58    1   0       114   318    0        2      140      0      4.4   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "101      0   0     3       1  \n",
      "204      0   3     3       0  \n",
      "221      0   0     3       0  \n",
      "250      1   3     3       0  \n",
      "291      0   3     1       0  \n",
      "Check the data has been processed correctly:\n",
      "        age  trestbps      chol   thalach   oldpeak  target  sex_1  cp_1  \\\n",
      "0  0.952197  0.828728 -0.255601  0.013543  1.150938       1      1     0   \n",
      "1 -1.915313 -0.077351  0.102487  1.641748  2.233684       1      1     0   \n",
      "2 -1.474158 -0.077351 -0.866457  0.981665  0.338879       1      0     1   \n",
      "3  0.180175 -0.681403 -0.192409  1.245698 -0.202494       1      1     1   \n",
      "4  0.290464 -0.681403  2.293143  0.585615 -0.382951       1      0     0   \n",
      "\n",
      "   cp_2  cp_3  ...  exang_1  slope_1  slope_2  ca_1  ca_2  ca_3  ca_4  thal_1  \\\n",
      "0     0     1  ...        0        0        0     0     0     0     0       1   \n",
      "1     1     0  ...        0        0        0     0     0     0     0       0   \n",
      "2     0     0  ...        0        0        1     0     0     0     0       0   \n",
      "3     0     0  ...        0        0        1     0     0     0     0       0   \n",
      "4     0     0  ...        1        0        1     0     0     0     0       0   \n",
      "\n",
      "   thal_2  thal_3  \n",
      "0       0       0  \n",
      "1       1       0  \n",
      "2       1       0  \n",
      "3       1       0  \n",
      "4       1       0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Accuracy: 0.8360655737704918\n",
      "Confusion Matrix:\n",
      "[[26  3]\n",
      " [ 7 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        29\n",
      "           1       0.89      0.78      0.83        32\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.84      0.84      0.84        61\n",
      "weighted avg       0.84      0.84      0.84        61\n",
      "\n",
      "Model saved to heart_data\\heart_disease_rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# Directory setup for data\n",
    "db_directory = \"heart_data\"\n",
    "os.makedirs(db_directory, exist_ok=True)\n",
    "\n",
    "# Path to CSV file \n",
    "csv_file_path = r'C:\\Users\\User\\Desktop\\ITDAA4-12 Project DV38MC178\\heart.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"DataFrame structure after reading CSV:\")\n",
    "print(df.head())\n",
    "print(\"DataFrame columns and data types after reading CSV:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Checking for missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Find outliers in numerical features\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for feature in numerical_features:\n",
    "    outliers = identify_outliers(df, feature)\n",
    "    print(f\"Outliers in {feature}:\")\n",
    "    print(outliers)\n",
    "\n",
    "# Cap outliers using IQR method\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "# Apply capping method to required columns\n",
    "for feature in numerical_features:\n",
    "    df = cap_outliers(df, feature)\n",
    "\n",
    "# Standardize numerical features after capping outliers\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "# Check the processed data\n",
    "print(\"Check the data has been processed correctly:\")\n",
    "print(df.head())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df.drop(columns='target')\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Test model and evaluate \n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Save the model to disk\n",
    "model_path = os.path.join(db_directory, 'heart_disease_rf_model.pkl')\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f'Model saved to {model_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
